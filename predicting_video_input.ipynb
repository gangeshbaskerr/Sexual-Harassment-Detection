{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ee7334-c2fa-4470-9f96-096519c90e07",
   "metadata": {},
   "source": [
    "# Predicting a video input\n",
    "The model takes a video as input, extract frames from it and detects whether the image contains any instance of Sexual Harassment and classifies the image into two class labels,\n",
    "\n",
    "0 - Healthy Environment<br>\n",
    "1 - Sexual Harassment detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff55bf-6fe2-4564-94fe-29fff9423611",
   "metadata": {},
   "source": [
    "### Importing necessary modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b3e941-0a95-4fd2-b344-c593b4b39f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ec824-5bcb-4189-b99a-2d5ac99b06be",
   "metadata": {},
   "source": [
    "### Loading the saved model\n",
    "The model saved as an output of 'Harassment_Detection_VGG16.ipynb' is loaded to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8cc4fbf-83ca-43d9-868d-ba1e58ed2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model(r'C:\\SUDHARSHAN\\Sexual_harassment_detection\\weight.hdf5')  # Make sure you have the correct path to your saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddce543-38e3-40b5-aaa3-333a5012f036",
   "metadata": {},
   "source": [
    "### Loading our base model\n",
    "This base model is pre-trained on the ImageNet dataset and is used to extract features from the video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e21113-7483-41c0-baa5-ba38e930a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 base model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fdccdd-7791-4d67-8f58-0f3090a73988",
   "metadata": {},
   "source": [
    "### Function to preprocess frames\n",
    "Takes an input video frame and preprocesses it for feeding into the VGG16 base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edc1c526-7239-4be7-889f-81b35bbc68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess a frame from the video\n",
    "def preprocess_frame(frame):\n",
    "    resized_frame = cv2.resize(frame, (224, 224))\n",
    "    preprocessed_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)  # VGG16 expects RGB images\n",
    "    preprocessed_frame = preprocess_input(preprocessed_frame)\n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186e624-f2d0-4b37-96ac-dd85627f75b1",
   "metadata": {},
   "source": [
    "### Video input and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb03ce26-f939-4821-881b-95b1023c4f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "video_path = r\"C:\\SUDHARSHAN\\Machine Learning\\Application-of-Neural-Networks-for-Detection-of-Sexual-Harassment-in-Workspace-main\\Application-of-Neural-Networks-for-Detection-of-Sexual-Harassment-in-Workspace-main\\Dataset\\Harassment\\Harassment_h20.mp4\"  # Replace with the path to your video file\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    preprocessed_frame = preprocess_frame(frame)\n",
    "\n",
    "    # Extract features using the VGG16 base model\n",
    "    new_image_features = base_model.predict(np.expand_dims(preprocessed_frame, axis=0))\n",
    "    new_image_features = new_image_features.reshape(1, 7 * 7 * 512)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(new_image_features)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = np.argmax(predictions[0])\n",
    "\n",
    "    # If your classes are labeled as 0 and 1, you can map the index back to class labels\n",
    "    class_labels = {0: 'Healthy Environment', 1: 'Sexual Harassment Detected'}\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "    # Display the predicted class label on the frame\n",
    "    cv2.putText(frame, predicted_class_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Video with Prediction\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
